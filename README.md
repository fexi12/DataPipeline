
# ETL Pipeline ğŸ“

This is my private repository, inspired by a career-related challenge.
It is still in `Development` stage. in this project you can see an ETL Pipeline, it extracts data from different public API - [NHTSA](https://www.nhtsa.gov/nhtsa-datasets-and-apis) , this one having various sources of an API or a ZiP File.


The language used was python and its different modules were:
-    Python 3.9
-    pyspark
-    pandas
-    configparser
-    json
-    sqlalchemy
-    concurrent.futures




# Project Tree

<pre>
ğŸ“¦NHTSA
 â”£ ğŸ“‚Complaints
 â”ƒ â”£ ğŸ“‚config
 â”ƒ â”ƒ â”— ğŸ“œconfig.ini
 â”ƒ â”£ ğŸ“‚in
 â”ƒ â”£ ğŸ“‚out
 â”ƒ â”£ ğŸ“œextract_complaints.py
 â”ƒ â”£ ğŸ“œload_complaints.py
 â”ƒ â”£ ğŸ“œmain_complaints.py
 â”ƒ â”— ğŸ“œtransform_complaints.py
 â”£ ğŸ“‚Investigations
 â”ƒ â”£ ğŸ“‚config
 â”ƒ â”ƒ â”— ğŸ“œ.env
 â”ƒ â”£ ğŸ“‚in
 â”ƒ â”£ ğŸ“‚out
 â”ƒ â”£ ğŸ“œextract_investigations.py
 â”ƒ â”£ ğŸ“œload_investigations.py
 â”ƒ â”£ ğŸ“œmain_investigations.py
 â”ƒ â”— ğŸ“œtransform_investigations.py
 â”£ ğŸ“‚ManufacturerCommunications
 â”ƒ â”£ ğŸ“‚config
 â”ƒ â”ƒ â”— ğŸ“œ.env
 â”ƒ â”£ ğŸ“‚in
 â”ƒ â”£ ğŸ“‚out
 â”£ ğŸ“‚Ratings
 â”ƒ â”£ ğŸ“‚config
 â”ƒ â”ƒ â”— ğŸ“œ.env
 â”ƒ â”£ ğŸ“‚in
 â”ƒ â”£ ğŸ“‚out
 â”ƒ â”£ ğŸ“œextract_ratings.py
 â”ƒ â”£ ğŸ“œload_ratings.py
 â”ƒ â”£ ğŸ“œmain_ratings.py
 â”ƒ â”— ğŸ“œtransform_ratings.py
 â”— ğŸ“‚Recalls
 â”ƒ â”£ ğŸ“‚config
 â”ƒ â”ƒ â”— ğŸ“œ.env
 â”ƒ â”£ ğŸ“‚in
 â”ƒ â”£ ğŸ“‚out
 â”ƒ â”£ ğŸ“œextract_recalls.py
 â”ƒ â”£ ğŸ“œload_recalls.py
 â”ƒ â”£ ğŸ“œmain_recalls.py
 â”ƒ â”— ğŸ“œtransform_recalls.py
</pre>

Each sub-folders is referring to [NHTSA](https://www.nhtsa.gov/nhtsa-datasets-and-apis) and it's Dataset Source.

<pre>
To Start a Pipeline you need to execute the command -> Â´Â´python .\main_*.py Â´Â´
Main*.py initialize the pipeline giving it's proper order :
 - extract.*py -> transform.*.py -> load.*py
</pre>
## Extract.*py

  ```txt
  - Retrieve the JSON from the API or the ZIP from the URL
  - Upload the sources to /in
  ```
## Transform.*.py

  ```txt
  - Get the file from /in
  - Transform to DataFrame via pyspark
  - Do transformation to the data
  - Transform to Pandas DataFrame
  - Upload the df to /out
  ```

## Load.*.py

  ```txt
  - Get the DataFrame from Transform.*py
  - Upload to a Database 
  ```
## .env
  ```txt
  - It holds the variables
  ```

<!-- ROADMAP -->
## Roadmap

- [x] Change the request to multiThread request ( to improve (reduce) the time of extract )
- [x] Create a .env file for the folders
- [x] Transform data to the correct format
- [x] Replace Null Values to empty
- [x] Upload the DataFrame Locally
- [ ] Continues to process data even if a url have no data
- [ ] Check the Data and it's transformation 
- [ ] Upload the DataFrame to MSQ Server
- [ ] Create DataFrame with index (missing someones)
- [ ] See if it's possible to create foreing keys in order to link the dataframes in the SQL
- [ ]
