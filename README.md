
# ETL Pipeline ðŸ“Ž

This is my private repository, inspired by a career-related challenge.
It is still in `Development` stage. in this project you can see an ETL Pipeline, it extracts data from different public API - [NHTSA](https://www.nhtsa.gov/nhtsa-datasets-and-apis) , this one having various sources of an API or a ZiP File.


The language used was python and its different modules were:
-    Python 3.9
-    pyspark
-    pandas
-    configparser
-    json
-    sqlalchemy
-    concurrent.futures




# Project Tree

<pre>
ðŸ“¦NHTSA
 â”£ ðŸ“‚Complaints
 â”ƒ â”£ ðŸ“‚config
 â”ƒ â”ƒ â”— ðŸ“œconfig.ini
 â”ƒ â”£ ðŸ“‚in
 â”ƒ â”£ ðŸ“‚out
 â”ƒ â”£ ðŸ“œextract_complaints.py
 â”ƒ â”£ ðŸ“œload_complaints.py
 â”ƒ â”£ ðŸ“œmain_complaints.py
 â”ƒ â”— ðŸ“œtransform_complaints.py
 â”£ ðŸ“‚Investigations
 â”ƒ â”£ ðŸ“‚config
 â”ƒ â”ƒ â”— ðŸ“œ.env
 â”ƒ â”£ ðŸ“‚in
 â”ƒ â”£ ðŸ“‚out
 â”ƒ â”£ ðŸ“œextract_investigations.py
 â”ƒ â”£ ðŸ“œload_investigations.py
 â”ƒ â”£ ðŸ“œmain_investigations.py
 â”ƒ â”— ðŸ“œtransform_investigations.py
 â”£ ðŸ“‚ManufacturerCommunications
 â”ƒ â”£ ðŸ“‚config
 â”ƒ â”ƒ â”— ðŸ“œ.env
 â”ƒ â”£ ðŸ“‚in
 â”ƒ â”£ ðŸ“‚out
 â”£ ðŸ“‚Ratings
 â”ƒ â”£ ðŸ“‚config
 â”ƒ â”ƒ â”— ðŸ“œ.env
 â”ƒ â”£ ðŸ“‚in
 â”ƒ â”£ ðŸ“‚out
 â”ƒ â”£ ðŸ“œextract_ratings.py
 â”ƒ â”£ ðŸ“œload_ratings.py
 â”ƒ â”£ ðŸ“œmain_ratings.py
 â”ƒ â”— ðŸ“œtransform_ratings.py
 â”— ðŸ“‚Recalls
 â”ƒ â”£ ðŸ“‚config
 â”ƒ â”ƒ â”— ðŸ“œ.env
 â”ƒ â”£ ðŸ“‚in
 â”ƒ â”£ ðŸ“‚out
 â”ƒ â”£ ðŸ“œextract_recalls.py
 â”ƒ â”£ ðŸ“œload_recalls.py
 â”ƒ â”£ ðŸ“œmain_recalls.py
 â”ƒ â”— ðŸ“œtransform_recalls.py
</pre>

Each sub-folders is referring to [NHTSA](https://www.nhtsa.gov/nhtsa-datasets-and-apis) and it's Dataset Source.

<pre>
To Start a Pipeline you need to execute the command -> Â´Â´python .\main_*.py Â´Â´
Main*.py initialize the pipeline giving it's proper order :
 - extract.*py -> transform.*.py -> load.*py
</pre>
## Extract.*py

  ```txt
  - Retrieve the JSON from the API or the ZIP from the URL
  - Upload the sources to /in
  ```
## Transform.*.py

  ```txt
  - Get the file from /in
  - Transform to DataFrame via pyspark
  - Do transformation to the data
  - Transform to Pandas DataFrame
  - Upload the df to /out
  ```

## Load.*.py

  ```txt
  - Get the DataFrame from Transform.*py
  - Upload to a Database 
  ```
## .env
  ```txt
  - It holds the variables
  ```

<!-- ROADMAP -->
## Roadmap

- [x] Change the request to multiThread request ( to improve (reduce) the time of extract )
- [x] Create a .env file for the folders
- [x] Transform data to the correct format
- [x] Replace Null Values to empty
- [x] Upload the DataFrame Locally
- [ ] Check the Data and it's transformation 
- [ ] Upload the DataFrame to MSQ Server
- [ ] Create DataFrame with index (missing someones)
- [ ] See if it's possible to create foreing keys in order to link the dataframes in the SQL

## Problems faced

- Understanding which data will be useful was challenging. I took an approach of retrieving all the data ( that was very high time consuming and delayed the development )
- Pipelines took too much time (hours) and made excessive requests to the API, leading to temporary API blocks.
- Limit on Storage to upload the SQL Lite DB and the ZIP and JSON on folder /in & /out
